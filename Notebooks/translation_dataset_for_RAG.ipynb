{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea510808-e890-4702-8202-40becd7a7103",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from datasets import load_dataset\n",
    "from langdetect import detect\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5f56ddd-dd76-4f42-ae9a-1eb23399db58",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 1200 records from MLSUM dataset for language: de\n",
      "Loading 1200 records from MLSUM dataset for language: es\n",
      "Loading 1200 records from MLSUM dataset for language: fr\n",
      "Loading 1200 records from MLSUM dataset for language: ru\n",
      "Loading 1200 records from MLSUM dataset for language: tu\n"
     ]
    }
   ],
   "source": [
    "# An empty list to store DataFrames for each language\n",
    "dataframes = []\n",
    "\n",
    "# Define the list of language configurations\n",
    "languages = ['de', 'es', 'fr', 'ru', 'tu']\n",
    "\n",
    "# Number of records to download for each language\n",
    "records_to_download = 1200\n",
    "\n",
    "# Load the dataset for each language configuration\n",
    "for lang in languages:\n",
    "    print(f\"Loading {records_to_download} records from MLSUM dataset for language: {lang}\")\n",
    "    dataset = load_dataset('mlsum', lang, split=f'train[:{records_to_download}]', trust_remote_code=True)\n",
    "\n",
    "    # Convert to pandas DataFrame for easier manipulation\n",
    "    df = pd.DataFrame(dataset)\n",
    "\n",
    "    # Add a column for language\n",
    "    df['language'] = lang\n",
    "\n",
    "    # Append the DataFrame to the list\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Concatenate all DataFrames into one\n",
    "data = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Filter to keep relevant columns\n",
    "data = data.drop('url', axis=1, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2160c9ac-e454-4c20-a368-83f40d294a1f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "      <th>topic</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Transport im Viehwaggon, Fleischgeruch in der ...</td>\n",
       "      <td>Transport im Viehwaggon, Fleischgeruch in der ...</td>\n",
       "      <td>politik</td>\n",
       "      <td>So war Auschwitz: Erinnerungen einer Holocaust...</td>\n",
       "      <td>00/01/2010</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Marmorner Zebrastreifen, pompöse Gebäude: Sind...</td>\n",
       "      <td>Marmorner Zebrastreifen, pompöse Gebäude: Sind...</td>\n",
       "      <td>politik</td>\n",
       "      <td>Kommunen in Not (3): Sindelfingen - Jenseits g...</td>\n",
       "      <td>00/01/2010</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wenn an diesem Montag die Landesvorsitzenden d...</td>\n",
       "      <td>Oskar Lafontaine gibt den Parteivorsitz der Li...</td>\n",
       "      <td>politik</td>\n",
       "      <td>Personaldebatte bei der Linken - Wer kommt nac...</td>\n",
       "      <td>00/01/2010</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Das Portrait von 1791 zeigt Haitis Nationalhel...</td>\n",
       "      <td>Die Wurzeln des Elends liegen in der Vergangen...</td>\n",
       "      <td>politik</td>\n",
       "      <td>Geschichte von Haiti - Napoleons Schmach</td>\n",
       "      <td>00/01/2010</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Neue Köpfe und alte Bekannte: Die neue Regieru...</td>\n",
       "      <td>Schwarz-Gelb ist noch nicht jene Traumkoalitio...</td>\n",
       "      <td>politik</td>\n",
       "      <td>Schwarz-gelbes Kabinett - Merkels Mannschaft i...</td>\n",
       "      <td>00/01/2010</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5995</th>\n",
       "      <td>Bodrum'da etkisini yitiren yağışlı hava, manda...</td>\n",
       "      <td>Muğla'nın Bodrum ilçesinde etkili olan olumsuz...</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Yağış mandalina bahçelerini vurdu</td>\n",
       "      <td>00/01/2010</td>\n",
       "      <td>tu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5996</th>\n",
       "      <td>Edinilen bilgiye göre, Adapazarı'ndan Bilecik ...</td>\n",
       "      <td>Sakarya'nın Pamukova ilçesindeki trafik kazası...</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Pamukovada trafik kazası</td>\n",
       "      <td>00/01/2010</td>\n",
       "      <td>tu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5997</th>\n",
       "      <td>- İstanbul kent genelinde Saat 18:18 itibari i...</td>\n",
       "      <td>Yoğun kar yağışı ve tipinin etkisi altındaki İ...</td>\n",
       "      <td>unknown</td>\n",
       "      <td>İstanbulda trafik durumu</td>\n",
       "      <td>00/01/2010</td>\n",
       "      <td>tu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5998</th>\n",
       "      <td>Tükettiğimiz gıdaların hijyenik ve temiz olmas...</td>\n",
       "      <td>işte güvenli ve sağlıklı yemek hazırlamak için...</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Güvenli yemek için 10 altın öneri</td>\n",
       "      <td>00/01/2010</td>\n",
       "      <td>tu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5999</th>\n",
       "      <td>Dink davası savcısının ilgisiz kaldığını öne s...</td>\n",
       "      <td>''Derin Devlet Davaları'nda Ne Durumdayız?'' p...</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Dinkin avukatından ilginç suçlama!</td>\n",
       "      <td>00/01/2010</td>\n",
       "      <td>tu</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "0     Transport im Viehwaggon, Fleischgeruch in der ...   \n",
       "1     Marmorner Zebrastreifen, pompöse Gebäude: Sind...   \n",
       "2     Wenn an diesem Montag die Landesvorsitzenden d...   \n",
       "3     Das Portrait von 1791 zeigt Haitis Nationalhel...   \n",
       "4     Neue Köpfe und alte Bekannte: Die neue Regieru...   \n",
       "...                                                 ...   \n",
       "5995  Bodrum'da etkisini yitiren yağışlı hava, manda...   \n",
       "5996  Edinilen bilgiye göre, Adapazarı'ndan Bilecik ...   \n",
       "5997  - İstanbul kent genelinde Saat 18:18 itibari i...   \n",
       "5998  Tükettiğimiz gıdaların hijyenik ve temiz olmas...   \n",
       "5999  Dink davası savcısının ilgisiz kaldığını öne s...   \n",
       "\n",
       "                                                summary    topic  \\\n",
       "0     Transport im Viehwaggon, Fleischgeruch in der ...  politik   \n",
       "1     Marmorner Zebrastreifen, pompöse Gebäude: Sind...  politik   \n",
       "2     Oskar Lafontaine gibt den Parteivorsitz der Li...  politik   \n",
       "3     Die Wurzeln des Elends liegen in der Vergangen...  politik   \n",
       "4     Schwarz-Gelb ist noch nicht jene Traumkoalitio...  politik   \n",
       "...                                                 ...      ...   \n",
       "5995  Muğla'nın Bodrum ilçesinde etkili olan olumsuz...  unknown   \n",
       "5996  Sakarya'nın Pamukova ilçesindeki trafik kazası...  unknown   \n",
       "5997  Yoğun kar yağışı ve tipinin etkisi altındaki İ...  unknown   \n",
       "5998  işte güvenli ve sağlıklı yemek hazırlamak için...  unknown   \n",
       "5999  ''Derin Devlet Davaları'nda Ne Durumdayız?'' p...  unknown   \n",
       "\n",
       "                                                  title        date language  \n",
       "0     So war Auschwitz: Erinnerungen einer Holocaust...  00/01/2010       de  \n",
       "1     Kommunen in Not (3): Sindelfingen - Jenseits g...  00/01/2010       de  \n",
       "2     Personaldebatte bei der Linken - Wer kommt nac...  00/01/2010       de  \n",
       "3              Geschichte von Haiti - Napoleons Schmach  00/01/2010       de  \n",
       "4     Schwarz-gelbes Kabinett - Merkels Mannschaft i...  00/01/2010       de  \n",
       "...                                                 ...         ...      ...  \n",
       "5995                  Yağış mandalina bahçelerini vurdu  00/01/2010       tu  \n",
       "5996                           Pamukovada trafik kazası  00/01/2010       tu  \n",
       "5997                           İstanbulda trafik durumu  00/01/2010       tu  \n",
       "5998                  Güvenli yemek için 10 altın öneri  00/01/2010       tu  \n",
       "5999                 Dinkin avukatından ilginç suçlama!  00/01/2010       tu  \n",
       "\n",
       "[6000 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5271ea62-710a-43e1-b5b6-f838fd0b708a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "347f5eb2-9772-4599-91bc-f5a5b30cae5b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'summary', 'topic', 'title', 'date', 'language'],\n",
       "    num_rows: 6000\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "829e8d71-031c-4370-ae18-cded987e15af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-04 19:28:10.222821: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-04 19:28:12.033172: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2024-12-04 19:28:12.033391: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2024-12-04 19:28:12.033403: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from transformers import MBartForConditionalGeneration, MBart50Tokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load translation model and tokenizer\n",
    "translation_model_name = \"facebook/mbart-large-50-many-to-one-mmt\"\n",
    "translation_model = MBartForConditionalGeneration.from_pretrained(translation_model_name)\n",
    "translation_tokenizer = MBart50Tokenizer.from_pretrained(translation_model_name, tgt_lang=\"en_XX\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63848a5-83aa-46cd-bddb-7f8cd40ac01c",
   "metadata": {},
   "source": [
    "#### Translate dataset to english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d1e8209-919e-45f2-b7aa-54ed5076f92f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a mapping of detected languages to MBART language codes\n",
    "lang_code_mapping = {\n",
    "    \"de\": \"de_DE\",  # German\n",
    "    \"es\": \"es_XX\",  # Spanish\n",
    "    \"fr\": \"fr_XX\",  # French\n",
    "    \"ru\": \"ru_RU\",  # Russian\n",
    "    \"tu\": \"tr_TR\"   # Turkish\n",
    "}\n",
    "\n",
    "# Define a function to translate text\n",
    "def translate_text(text):\n",
    "    try:\n",
    "        # Detect the language\n",
    "        detected_lang = detect(text)\n",
    "        # Map detected language to MBART code, default to English if not found\n",
    "        lang_code = lang_code_mapping.get(detected_lang, \"en_XX\")\n",
    "        # Set the source language dynamically\n",
    "        translation_tokenizer.src_lang = lang_code\n",
    "        inputs = translation_tokenizer(text, return_tensors=\"pt\", max_length=128, truncation=True)\n",
    "        outputs = translation_model.generate(**inputs)\n",
    "        translation = translation_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return translation\n",
    "    except Exception as e:\n",
    "        print(f\"Error translating text: {text[:30]}... - {e}\")\n",
    "        return text  # Return the original text if translation fails\n",
    "    \n",
    "# Translate the dataset\n",
    "def translate_dataset(dataset):\n",
    "    translated_texts = []\n",
    "    for text in tqdm(dataset[\"text\"], desc=\"Translating Texts\"):\n",
    "        translated_text = translate_text(text)\n",
    "        translated_texts.append(translated_text)\n",
    "    return translated_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca91b0a-4fa2-4e18-855f-e881881b339f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "translated_texts = translate_dataset(dataset)\n",
    "dataset = dataset.add_column(\"translated_text\", translated_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "773cb49e-aeed-4519-9592-be4fe24f259b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "443e92a5917c4a5ebb3d088a4fc16b11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/6000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All columns translated and dataset saved!\n"
     ]
    }
   ],
   "source": [
    "# Save the translated dataset\n",
    "dataset.save_to_disk(\"./fully_translated_dataset_v2\")\n",
    "print(\"All columns translated and dataset saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acdc438-6ac3-4868-80ce-be326476a228",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "translated_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c3e3f0-ec9d-4ca1-9721-950077774523",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# export GOOGLE_APPLICATION_CREDENTIALS=\"nlp_proj/service-account-key/nlp-translate-summarize-ea64d2a44317.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9612a127-9c3c-4cf8-aee1-8e93ef26b08f",
   "metadata": {},
   "source": [
    "### compress and upload to gcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d355b381-a381-459b-b883-ec7020217174",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69f97e1b7ae0472f8e852ca8fbafe083",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/6000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./\n",
      "./state.json\n",
      "./data-00000-of-00001.arrow\n",
      "./dataset_info.json\n",
      "File translated_dataset_v2.tar.gz uploaded to datasets/translated_dataset_v2.tar.gz.\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import storage\n",
    "from datasets import load_from_disk\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Load the translated dataset\n",
    "translated_dataset_path = \"./fully_translated_dataset_v2\"\n",
    "translated_dataset = load_from_disk(translated_dataset_path)\n",
    "\n",
    "# Create a new directory for saving the compressed dataset\n",
    "compressed_dataset_dir = \"./compressed_translated_dataset_v2\"\n",
    "if os.path.exists(compressed_dataset_dir):\n",
    "    shutil.rmtree(compressed_dataset_dir)  # Remove the directory if it already exists\n",
    "os.makedirs(compressed_dataset_dir, exist_ok=True)\n",
    "\n",
    "# Save the dataset to the new directory\n",
    "translated_dataset.save_to_disk(compressed_dataset_dir)\n",
    "\n",
    "# Compress the directory into a tar.gz file\n",
    "compressed_file = \"translated_dataset_v2.tar.gz\"\n",
    "os.system(f\"tar -czvf {compressed_file} -C {compressed_dataset_dir} .\")\n",
    "\n",
    "# Upload to GCS\n",
    "def upload_to_gcs(bucket_name, source_file_name, destination_blob_name):\n",
    "    \"\"\"Uploads a file to a GCS bucket.\"\"\"\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "\n",
    "    blob.upload_from_filename(source_file_name)\n",
    "    print(f\"File {source_file_name} uploaded to {destination_blob_name}.\")\n",
    "\n",
    "# Define GCS bucket and file details\n",
    "bucket_name = \"nlp_proj\"  # Replace with your bucket name\n",
    "destination_blob_name = \"datasets/translated_dataset_v2.tar.gz\"  # Desired path in GCS\n",
    "\n",
    "# Upload the dataset to GCS\n",
    "upload_to_gcs(bucket_name, compressed_file, destination_blob_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5adbc5-60ec-43a2-9610-2db57776ec4e",
   "metadata": {},
   "source": [
    "### load and unzip data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e00e2ada-48cb-42f3-a116-aacaa7384160",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File datasets/translated_dataset_v2.tar.gz downloaded to translated_dataset.tar_v2.gz.\n",
      "Extracted translated_dataset.tar_v2.gz to ./extracted_translated_dataset_v2.\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import storage\n",
    "import os\n",
    "import tarfile\n",
    "\n",
    "# Define GCS bucket and file details\n",
    "bucket_name = \"nlp_proj\"  # Replace with your bucket name\n",
    "source_blob_name = \"datasets/translated_dataset_v2.tar.gz\"  # Path in GCS\n",
    "destination_file_name = \"translated_dataset.tar_v2.gz\"  # Local file name\n",
    "\n",
    "# Function to download a file from GCS\n",
    "def download_from_gcs(bucket_name, source_blob_name, destination_file_name):\n",
    "    \"\"\"Downloads a file from GCS bucket.\"\"\"\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(source_blob_name)\n",
    "\n",
    "    blob.download_to_filename(destination_file_name)\n",
    "    print(f\"File {source_blob_name} downloaded to {destination_file_name}.\")\n",
    "\n",
    "# Download the file from GCS\n",
    "download_from_gcs(bucket_name, source_blob_name, destination_file_name)\n",
    "\n",
    "# Unzip the downloaded tar.gz file\n",
    "def extract_tar_gz(file_name, extract_path):\n",
    "    \"\"\"Extracts a tar.gz file to the specified path.\"\"\"\n",
    "    with tarfile.open(file_name, \"r:gz\") as tar:\n",
    "        tar.extractall(path=extract_path)\n",
    "    print(f\"Extracted {file_name} to {extract_path}.\")\n",
    "\n",
    "# Define extraction path\n",
    "extracted_path = \"./extracted_translated_dataset_v2\"\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(extracted_path, exist_ok=True)\n",
    "\n",
    "# Extract the dataset\n",
    "extract_tar_gz(destination_file_name, extracted_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "935849fe-ccef-4a90-bece-3cca65d958b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'summary', 'topic', 'title', 'date', 'language', 'translated_text'],\n",
      "    num_rows: 6000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "# Load the dataset from the extracted path\n",
    "dataset = load_from_disk(extracted_path)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43a7c9ab-5ee3-4e35-9ab2-f5807915f32e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0  Transport im Viehwaggon, Fleischgeruch in der ...   \n",
      "1  Marmorner Zebrastreifen, pompöse Gebäude: Sind...   \n",
      "2  Wenn an diesem Montag die Landesvorsitzenden d...   \n",
      "3  Das Portrait von 1791 zeigt Haitis Nationalhel...   \n",
      "4  Neue Köpfe und alte Bekannte: Die neue Regieru...   \n",
      "\n",
      "                                             summary    topic  \\\n",
      "0  Transport im Viehwaggon, Fleischgeruch in der ...  politik   \n",
      "1  Marmorner Zebrastreifen, pompöse Gebäude: Sind...  politik   \n",
      "2  Oskar Lafontaine gibt den Parteivorsitz der Li...  politik   \n",
      "3  Die Wurzeln des Elends liegen in der Vergangen...  politik   \n",
      "4  Schwarz-Gelb ist noch nicht jene Traumkoalitio...  politik   \n",
      "\n",
      "                                               title        date language  \\\n",
      "0  So war Auschwitz: Erinnerungen einer Holocaust...  00/01/2010       de   \n",
      "1  Kommunen in Not (3): Sindelfingen - Jenseits g...  00/01/2010       de   \n",
      "2  Personaldebatte bei der Linken - Wer kommt nac...  00/01/2010       de   \n",
      "3           Geschichte von Haiti - Napoleons Schmach  00/01/2010       de   \n",
      "4  Schwarz-gelbes Kabinett - Merkels Mannschaft i...  00/01/2010       de   \n",
      "\n",
      "                                     translated_text  \n",
      "0  Transport in a cattle carriage, smell of meat ...  \n",
      "1  Marble zebra stripes, pompous buildings: Sinde...  \n",
      "2  This Monday, when the country’s left-wing lead...  \n",
      "3  The portrait of 1791 shows Haiti’s national he...  \n",
      "4  New heads and old acquaintances: Angela Merkel...  \n"
     ]
    }
   ],
   "source": [
    "df = dataset.to_pandas()\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e17aef83-a493-4b33-969b-fb77462143d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "\n",
    "# # Define a function to translate an entire column\n",
    "# def translate_column(column_data):\n",
    "#     translated_column = []\n",
    "#     for value in tqdm(column_data, desc=f\"Translating Column\"):\n",
    "#         if isinstance(value, str) and value.strip():  # Translate only non-empty strings\n",
    "#             translated_value = translate_text(value)\n",
    "#         else:\n",
    "#             translated_value = value  # Leave non-string or empty values as is\n",
    "#         translated_column.append(translated_value)\n",
    "#     return translated_column\n",
    "\n",
    "# # Translate only the specified columns\n",
    "# columns_to_translate = ['summary', 'topic', 'title']\n",
    "# translated_columns = {}\n",
    "\n",
    "# for column_name in columns_to_translate:\n",
    "#     print(f\"Translating column: {column_name}\")\n",
    "#     translated_columns[column_name] = translate_column(dataset[column_name])\n",
    "\n",
    "# # Create a new dataset with the desired columns\n",
    "# desired_columns = ['summary', 'topic', 'title', 'date', 'translated_text']\n",
    "# final_dataset_dict = {column: dataset[column] for column in desired_columns if column != 'translated_text'}\n",
    "\n",
    "# # Add translated columns to the final dataset dictionary\n",
    "# for column_name, translated_data in translated_columns.items():\n",
    "#     final_dataset_dict[column_name] = translated_data\n",
    "\n",
    "# # Convert the final dataset dictionary to a Dataset object\n",
    "# from datasets import Dataset\n",
    "# final_dataset = Dataset.from_dict(final_dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a046adb1-6c33-429d-94f3-05726635b9c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating column: summary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating Column: 100%|██████████| 3000/3000 [2:40:32<00:00,  3.21s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating column: topic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating Column: 100%|██████████| 3000/3000 [2:24:55<00:00,  2.90s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating column: title\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating Column: 100%|██████████| 3000/3000 [2:15:53<00:00,  2.72s/it]  \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "\n",
    "# Define a function to translate an entire column\n",
    "def translate_column(column_data):\n",
    "    translated_column = []\n",
    "    for value in tqdm(column_data, desc=f\"Translating Column\"):\n",
    "        if isinstance(value, str) and value.strip():  # Translate only non-empty strings\n",
    "            translated_value = translate_text(value)\n",
    "        else:\n",
    "            translated_value = value  # Leave non-string or empty values as is\n",
    "        translated_column.append(translated_value)\n",
    "    return translated_column\n",
    "\n",
    "# Define the number of records to process\n",
    "record_limit = 3000\n",
    "\n",
    "# Slice the dataset to limit the number of records\n",
    "dataset_limited = dataset.select(range(record_limit))\n",
    "\n",
    "# Translate the 'translated_text' column if it's not already translated\n",
    "if \"translated_text\" not in dataset_limited.column_names:\n",
    "    print(\"Translating 'text' column to create 'translated_text'...\")\n",
    "    translated_text_column = translate_column(dataset_limited[\"text\"])\n",
    "else:\n",
    "    translated_text_column = dataset_limited[\"translated_text\"]\n",
    "\n",
    "# Translate only the specified columns\n",
    "columns_to_translate = ['summary', 'topic', 'title']\n",
    "translated_columns = {}\n",
    "\n",
    "for column_name in columns_to_translate:\n",
    "    print(f\"Translating column: {column_name}\")\n",
    "    translated_columns[column_name] = translate_column(dataset_limited[column_name])\n",
    "\n",
    "# Create a new dataset with the desired columns\n",
    "desired_columns = ['summary', 'topic', 'title', 'date', 'translated_text']\n",
    "final_dataset_dict = {column: dataset_limited[column] for column in desired_columns if column != 'translated_text'}\n",
    "\n",
    "# Add translated columns to the final dataset dictionary\n",
    "for column_name, translated_data in translated_columns.items():\n",
    "    final_dataset_dict[column_name] = translated_data\n",
    "\n",
    "# Add the translated_text column\n",
    "final_dataset_dict[\"translated_text\"] = translated_text_column\n",
    "\n",
    "# Convert the final dataset dictionary to a Dataset object\n",
    "final_dataset = Dataset.from_dict(final_dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "17c08b7b-98f4-46c8-bb7e-3dadd5e35165",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27bee0e02eb74c52b20fb8b8a194e7c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specified columns translated, final dataset saved!\n"
     ]
    }
   ],
   "source": [
    "# Save the final dataset\n",
    "final_dataset.save_to_disk(\"./final_translated_dataset_v3\")\n",
    "print(\"Specified columns translated, final dataset saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6b3a33-9ffc-4501-b2f3-83a097f9fa23",
   "metadata": {},
   "source": [
    "### compress and upload to gcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "640ba7bc-9f40-4a85-a29a-73c31dc61502",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5d5c06b7dbc4647bee01a9339cb03c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./\n",
      "./state.json\n",
      "./data-00000-of-00001.arrow\n",
      "./dataset_info.json\n",
      "File translated_dataset_v3.tar.gz uploaded to datasets/translated_dataset_v3.tar.gz.\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import storage\n",
    "from datasets import load_from_disk\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Load the translated dataset\n",
    "translated_dataset_path = \"./final_translated_dataset_v3\"\n",
    "translated_dataset = load_from_disk(translated_dataset_path)\n",
    "\n",
    "# Create a new directory for saving the compressed dataset\n",
    "compressed_dataset_dir = \"./compressed_translated_dataset_v3\"\n",
    "if os.path.exists(compressed_dataset_dir):\n",
    "    shutil.rmtree(compressed_dataset_dir)  # Remove the directory if it already exists\n",
    "os.makedirs(compressed_dataset_dir, exist_ok=True)\n",
    "\n",
    "# Save the dataset to the new directory\n",
    "translated_dataset.save_to_disk(compressed_dataset_dir)\n",
    "\n",
    "# Compress the directory into a tar.gz file\n",
    "compressed_file = \"translated_dataset_v3.tar.gz\"\n",
    "os.system(f\"tar -czvf {compressed_file} -C {compressed_dataset_dir} .\")\n",
    "\n",
    "# Upload to GCS\n",
    "def upload_to_gcs(bucket_name, source_file_name, destination_blob_name):\n",
    "    \"\"\"Uploads a file to a GCS bucket.\"\"\"\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "\n",
    "    blob.upload_from_filename(source_file_name)\n",
    "    print(f\"File {source_file_name} uploaded to {destination_blob_name}.\")\n",
    "\n",
    "# Define GCS bucket and file details\n",
    "bucket_name = \"nlp_proj\"  # Replace with your bucket name\n",
    "destination_blob_name = \"datasets/translated_dataset_v3.tar.gz\"  # Desired path in GCS\n",
    "\n",
    "# Upload the dataset to GCS\n",
    "upload_to_gcs(bucket_name, compressed_file, destination_blob_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e653afb8-fb3e-4ceb-b2de-6137873c94c9",
   "metadata": {},
   "source": [
    "### load and unzip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ed587185-930d-413f-93b3-2ff776f424bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File datasets/translated_dataset_v3.tar.gz downloaded to translated_dataset.tar_v3.gz.\n",
      "Extracted translated_dataset.tar_v3.gz to ./extracted_translated_dataset_v3.\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import storage\n",
    "import os\n",
    "import tarfile\n",
    "\n",
    "# Define GCS bucket and file details\n",
    "bucket_name = \"nlp_proj\"  # Replace with your bucket name\n",
    "source_blob_name = \"datasets/translated_dataset_v3.tar.gz\"  # Path in GCS\n",
    "destination_file_name = \"translated_dataset.tar_v3.gz\"  # Local file name\n",
    "\n",
    "# Function to download a file from GCS\n",
    "def download_from_gcs(bucket_name, source_blob_name, destination_file_name):\n",
    "    \"\"\"Downloads a file from GCS bucket.\"\"\"\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(source_blob_name)\n",
    "\n",
    "    blob.download_to_filename(destination_file_name)\n",
    "    print(f\"File {source_blob_name} downloaded to {destination_file_name}.\")\n",
    "\n",
    "# Download the file from GCS\n",
    "download_from_gcs(bucket_name, source_blob_name, destination_file_name)\n",
    "\n",
    "# Unzip the downloaded tar.gz file\n",
    "def extract_tar_gz(file_name, extract_path):\n",
    "    \"\"\"Extracts a tar.gz file to the specified path.\"\"\"\n",
    "    with tarfile.open(file_name, \"r:gz\") as tar:\n",
    "        tar.extractall(path=extract_path)\n",
    "    print(f\"Extracted {file_name} to {extract_path}.\")\n",
    "\n",
    "# Define extraction path\n",
    "extracted_path = \"./extracted_translated_dataset_v3\"\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(extracted_path, exist_ok=True)\n",
    "\n",
    "# Extract the dataset\n",
    "extract_tar_gz(destination_file_name, extracted_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1593831d-2ed9-46e5-a30c-58dfccdd11cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['summary', 'topic', 'title', 'date', 'translated_text'],\n",
      "    num_rows: 3000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "# Load the dataset from the extracted path\n",
    "dataset = load_from_disk(extracted_path)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "80ce5ed0-ca24-4395-a17c-a84aaa7f6c4f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': ['Transport in a cattle carriage, smell of meat in the air, selection with Dr Mengele: Holocaust survivor Lisa Miková remembers the Auschwitz-Birkenau extermination camp.'],\n",
       " 'topic': ['Politics'],\n",
       " 'title': ['Auschwitz: Memories of a Holocaust Survivor'],\n",
       " 'date': ['00/01/2010'],\n",
       " 'translated_text': ['Transport in a cattle carriage, smell of meat in the air, selection with Dr Mengele: Holocaust survivor Lisa Miková remembers the Auschwitz-Birkenau extermination camp. Lisa Miková was born in 1922 in Prague. In her largely secular Jewish family, German and Czech were spoken. In 1942, she was deported to the Theresienstadt concentration camp. From there, her parents were first deported, then, in the autumn of 1944, her husband František to the Auschwitz extermination camp. Lisa Miková volunteered shortly afterwards.']}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469c13a3-62ff-46eb-8d97-b010082eb650",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-cpu.2-11:m125"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
