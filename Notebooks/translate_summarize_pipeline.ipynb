{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c45d67e8-2b8a-40f8-a51b-5b386aee0f3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from datasets import load_dataset\n",
    "from langdetect import detect\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b9d7d71-e5ad-4d69-971d-0ee2d8e7af63",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 700 records from MLSUM dataset for language: de\n",
      "Loading 700 records from MLSUM dataset for language: es\n",
      "Loading 700 records from MLSUM dataset for language: fr\n",
      "Loading 700 records from MLSUM dataset for language: ru\n",
      "Loading 700 records from MLSUM dataset for language: tu\n",
      "Filtered Data with Language Detection:\n",
      "                                                text  \\\n",
      "0  Transport im Viehwaggon, Fleischgeruch in der ...   \n",
      "1  Marmorner Zebrastreifen, pompöse Gebäude: Sind...   \n",
      "2  Wenn an diesem Montag die Landesvorsitzenden d...   \n",
      "\n",
      "                                             summary    topic  \\\n",
      "0  Transport im Viehwaggon, Fleischgeruch in der ...  politik   \n",
      "1  Marmorner Zebrastreifen, pompöse Gebäude: Sind...  politik   \n",
      "2  Oskar Lafontaine gibt den Parteivorsitz der Li...  politik   \n",
      "\n",
      "                                               title        date language  \n",
      "0  So war Auschwitz: Erinnerungen einer Holocaust...  00/01/2010       de  \n",
      "1  Kommunen in Not (3): Sindelfingen - Jenseits g...  00/01/2010       de  \n",
      "2  Personaldebatte bei der Linken - Wer kommt nac...  00/01/2010       de  \n"
     ]
    }
   ],
   "source": [
    "# An empty list to store DataFrames for each language\n",
    "dataframes = []\n",
    "\n",
    "# Define the list of language configurations\n",
    "languages = ['de', 'es', 'fr', 'ru', 'tu']\n",
    "\n",
    "# Number of records to download for each language\n",
    "records_to_download = 700\n",
    "\n",
    "# Load the dataset for each language configuration\n",
    "for lang in languages:\n",
    "    print(f\"Loading {records_to_download} records from MLSUM dataset for language: {lang}\")\n",
    "    dataset = load_dataset('mlsum', lang, split=f'train[:{records_to_download}]', trust_remote_code=True)\n",
    "\n",
    "    # Convert to pandas DataFrame for easier manipulation\n",
    "    df = pd.DataFrame(dataset)\n",
    "\n",
    "    # Add a column for language\n",
    "    df['language'] = lang\n",
    "\n",
    "    # Append the DataFrame to the list\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Concatenate all DataFrames into one\n",
    "data = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Filter to keep relevant columns\n",
    "data = data.drop('url', axis=1, errors='ignore')\n",
    "\n",
    "# Display the filtered DataFrame\n",
    "print(\"Filtered Data with Language Detection:\")\n",
    "print(data.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dd2fcef-966f-4470-898c-241772fb626d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create Hugging Face Dataset\n",
    "dataset = Dataset.from_pandas(data)\n",
    "\n",
    "# Split dataset for training/evaluation (80% train, 20% validation)\n",
    "dataset = dataset.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "659b2ea4-e998-4580-9b38-17a98b8beeb5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-03 07:09:46.551813: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-03 07:09:47.623313: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2024-12-03 07:09:47.623451: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2024-12-03 07:09:47.623463: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from transformers import MBartForConditionalGeneration, MBart50Tokenizer\n",
    "\n",
    "# Load translation model and tokenizer\n",
    "translation_model_name = \"facebook/mbart-large-50-many-to-one-mmt\"\n",
    "translation_model = MBartForConditionalGeneration.from_pretrained(translation_model_name)\n",
    "translation_tokenizer = MBart50Tokenizer.from_pretrained(translation_model_name, src_lang=\"en_XX\", tgt_lang=\"en_XX\")\n",
    "\n",
    "def translate_text(text):\n",
    "    prompt = f\"Translate the text to English: {text}\"\n",
    "    inputs = translation_tokenizer(prompt, return_tensors=\"pt\", max_length=128, truncation=True)\n",
    "    outputs = translation_model.generate(**inputs)\n",
    "    translation = translation_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a218403-134e-46e5-af0b-f4f5530060f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating Summaries: 100%|██████████| 2800/2800 [1:42:13<00:00,  2.19s/it]  \n",
      "Translating Summaries: 100%|██████████| 700/700 [26:45<00:00,  2.29s/it]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd644531226c4f76b800e66bba482280",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/2800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec19e4b180804e77846d5068a8664de9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/700 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the translation function to translate the summaries\n",
    "def translate_summaries(dataset):\n",
    "    translated_summaries = []\n",
    "    for summary in tqdm(dataset[\"summary\"], desc=\"Translating Summaries\"):\n",
    "        translated_summary = translate_text(summary)\n",
    "        translated_summaries.append(translated_summary)\n",
    "    return translated_summaries\n",
    "\n",
    "# Translate the summaries in the dataset\n",
    "translated_summaries_train = translate_summaries(dataset[\"train\"])\n",
    "translated_summaries_test = translate_summaries(dataset[\"test\"])\n",
    "\n",
    "# Now add the translated summaries as a new column\n",
    "dataset[\"train\"] = dataset[\"train\"].add_column(\"translated_summary\", translated_summaries_train)\n",
    "dataset[\"test\"] = dataset[\"test\"].add_column(\"translated_summary\", translated_summaries_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "debe63d4-f690-4e10-80bc-430912cfb6c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating Input Texts: 100%|██████████| 175/175 [5:29:34<00:00, 112.99s/it]  \n",
      "Translating Input Texts: 100%|██████████| 44/44 [1:26:11<00:00, 117.53s/it]\n"
     ]
    }
   ],
   "source": [
    "# Define the translation function\n",
    "def translate_texts(texts, batch_size=16):\n",
    "    \"\"\"\n",
    "    Translate a list of texts in batches.\n",
    "    \n",
    "    Args:\n",
    "        texts (list): List of strings to translate.\n",
    "        batch_size (int): Number of texts to translate at a time.\n",
    "\n",
    "    Returns:\n",
    "        list: Translated texts.\n",
    "    \"\"\"\n",
    "    translated_texts = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Translating Input Texts\"):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        # Assume translate_text is a function that translates a single text\n",
    "        translated_batch = [translate_text(text) for text in batch]\n",
    "        translated_texts.extend(translated_batch)\n",
    "    return translated_texts\n",
    "\n",
    "# Translate the input texts in the train and test datasets\n",
    "translated_texts_train = translate_texts(dataset[\"train\"][\"text\"])\n",
    "translated_texts_test = translate_texts(dataset[\"test\"][\"text\"])\n",
    "\n",
    "# Add the translated input texts as a new column\n",
    "dataset[\"train\"] = dataset[\"train\"].add_column(\"translated_text\", translated_texts_train)\n",
    "dataset[\"test\"] = dataset[\"test\"].add_column(\"translated_text\", translated_texts_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71105836-fac6-40d3-9bc1-d3079657497f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text', 'summary', 'topic', 'title', 'date', 'language', 'translated_summary', 'translated_text']\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8c4692c-360a-43b3-950d-343ff17f9c25",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc8f94d737804e6dac7e4cd9b18c63c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eb7975ca2b3433aaa333a62061d3268",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/700 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_data_for_summarization(examples):\n",
    "    return {\n",
    "        \"input_text\": [f\"Summarize the following text: {text}\" for text in examples[\"translated_text\"]],\n",
    "        \"target_text\": examples[\"translated_summary\"]  # Use the translated summary here\n",
    "    }\n",
    "\n",
    "# Apply the preprocessing function to the dataset\n",
    "summarization_dataset = dataset.map(preprocess_data_for_summarization, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8b65d45-3815-4db7-8dc0-168b9eea0845",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# (summarization_dataset[\"train\"][\"translated_summary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b41fdb9-b76c-48f7-a86b-6ff41f141aa1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4868825f6086496f84c7f08aca85dbe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93324d8828d440bb9f06a53c8c32ad66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/700 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, PeftType, TaskType\n",
    "from transformers import TrainingArguments, Trainer, AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# Load summarization model and tokenizer\n",
    "summarization_model_name = \"t5-small\"  # Use T5 or any other Seq2Seq model\n",
    "summarization_model = AutoModelForSeq2SeqLM.from_pretrained(summarization_model_name)\n",
    "summarization_tokenizer = AutoTokenizer.from_pretrained(summarization_model_name)\n",
    "\n",
    "# Define LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q\", \"v\"],  # LoRA applied to specific layers\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM\n",
    ")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "lora_model = get_peft_model(summarization_model, lora_config)\n",
    "\n",
    "# Tokenize data for training\n",
    "def tokenize_function(examples):\n",
    "    inputs = summarization_tokenizer(\n",
    "        examples[\"input_text\"], max_length=512, truncation=True, padding=\"max_length\", return_tensors=\"pt\"\n",
    "    )\n",
    "    labels = summarization_tokenizer(\n",
    "        examples[\"target_text\"], max_length=128, truncation=True, padding=\"max_length\", return_tensors=\"pt\"\n",
    "    )[\"input_ids\"]\n",
    "    inputs[\"labels\"] = labels\n",
    "    return inputs\n",
    "\n",
    "tokenized_dataset = summarization_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61a48b7d-30b2-48e5-8ebb-21f6676c38d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tokenized_dataset[\"train\"]['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1fb2864c-1346-4c7c-bef5-7fee6e97bc7e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/var/tmp/ipykernel_129512/1657301929.py:15: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2100' max='2100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2100/2100 1:13:45, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.310400</td>\n",
       "      <td>0.925675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.924700</td>\n",
       "      <td>0.818257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.837200</td>\n",
       "      <td>0.770432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.818700</td>\n",
       "      <td>0.744333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.834000</td>\n",
       "      <td>0.733954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.798200</td>\n",
       "      <td>0.730913</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2100, training_loss=1.7337668827601842, metrics={'train_runtime': 4427.5871, 'train_samples_per_second': 3.794, 'train_steps_per_second': 0.474, 'total_flos': 2288962555084800.0, 'train_loss': 1.7337668827601842, 'epoch': 6.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./summarization_lora_results\",\n",
    "    evaluation_strategy=\"epoch\",  # Evaluate at the end of each epoch\n",
    "    save_strategy=\"epoch\",  # Save at the end of each epoch\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=6,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    load_best_model_at_end=True,  # Load the best model at the end of training\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=lora_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=summarization_tokenizer,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "87711e70-6c55-43c0-a731-866a2b02e765",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pipeline(input_text):\n",
    "    \n",
    "    # Translate text\n",
    "    translated_text = translate_text(input_text)\n",
    "    print(\"Translated text: \",translated_text)\n",
    "    \n",
    "    # Summarize translated text\n",
    "    prompt = f\"summarize this text: {translated_text}\"\n",
    "    inputs = summarization_tokenizer(\n",
    "            prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=512,\n",
    "            truncation=True\n",
    "        )    \n",
    "    # Generate summary\n",
    "    outputs = lora_model.generate(\n",
    "        **inputs,\n",
    "        max_length=150,\n",
    "        min_length=20,\n",
    "        num_beams=4,\n",
    "        no_repeat_ngram_size=2,\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "    summary = summarization_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a0d4fbf8-52ae-4a89-aa1d-df3ee15d28ec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated text:  The sun rose gently on the horizon, casting a golden light over the rolling hills. The birds began to sing, their melody filling the fresh morning air. In a small village nestled at the foot of the mountains, the inhabitants were already busy with their daily tasks. Children ran through the cobbled streets, laughing and playing hide-and-seek, while adults opened the sides of their stone houses.\n",
      "\n",
      "Summary: children ran through the cobbled streets, laughing and playing hide-and-seek, while adults opened the sides of their stone houses.\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "example_text = \"\"\"Le soleil se levait doucement à l'horizon, projetant une lumière dorée sur les collines ondulantes. Les oiseaux commençaient à chanter, leur mélodie emplissant l'air frais du matin. Dans un petit village niché au pied des montagnes, les habitants s'affairaient déjà à leurs tâches quotidiennes. Les enfants couraient dans les ruelles pavées, riant et jouant à cache-cache, tandis que les adultes ouvraient les volets de leurs maisons en pierre.\n",
    "\n",
    "Marie, une jeune femme aux cheveux bruns et aux yeux pétillants, sortit de sa maison en tenant un panier rempli de fleurs fraîches. Elle aimait commencer sa journée en cueillant des fleurs dans les champs voisins. Les couleurs vives des coquelicots, des marguerites et des bleuets illuminaient le paysage. Tandis qu'elle marchait, elle salua ses voisins avec un sourire chaleureux.\n",
    "\n",
    "Au centre du village se trouvait une petite place animée où les commerçants installaient leurs étals. Le boulanger, avec sa toque blanche et ses mains enfarinées, sortait des baguettes croustillantes du four. L'odeur du pain chaud se mêlait aux arômes des fruits frais et des herbes vendues par les marchands. Les discussions animées des villageois créaient une ambiance joyeuse et conviviale.\n",
    "\n",
    "Non loin de là, un vieux moulin à eau tournait lentement, alimenté par un ruisseau limpide. Les enfants aimaient s'y rassembler pour jeter des cailloux dans l'eau ou observer les poissons qui nageaient sous la surface. Les plus âgés racontaient des histoires sur les légendes locales, parlant de chevaliers courageux et de trésors cachés dans les montagnes environnantes.\n",
    "\n",
    "À la lisière du village, un berger conduisait son troupeau de moutons vers les pâturages verdoyants. Le tintement des clochettes attachées aux cous des animaux résonnait dans l'air tranquille. Les chiens de berger, alertes et obéissants, veillaient à ce qu'aucun mouton ne s'éloigne. Le berger, avec son bâton en bois et son chapeau de paille, semblait en parfaite harmonie avec la nature.\n",
    "\n",
    "Pendant ce temps, dans une petite ferme à la périphérie du village, un fermier et sa femme travaillaient dans leur potager. Ils plantaient des légumes et arrosaient les plantes sous le regard curieux d'un chat paresseux qui somnolait sur une\"\"\"\n",
    "result = pipeline(example_text)\n",
    "print(\"\\nSummary:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "738301a2-00ec-4fa8-bc2b-54cd263fe84d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated text:  Washington - As the Congress returns to its final activity before the session ends this week, it faces a crucial deadline of December 20 to stop the government shutdown. The Democrats and the Republicans seem to have resigned to pass a sustained motion, or CR, that will provide temporary financing to the government early in 2025 - possibly in March - because they have no time to make a full financing deal this year.\n",
      "\n",
      "Summary: Democrats and the Republicans seem to have resigned to pass a sustained motion, or CR, that will provide temporary financing to the government early in 2025.\n"
     ]
    }
   ],
   "source": [
    "example_text = \"\"\"वाशिंगटन - जैसा कि कांग्रेस इस सप्ताह सत्र समाप्त होने से पहले आखिरी गतिविधि के लिए लौटी है, उसे सरकारी शटडाउन को रोकने के लिए 20 दिसंबर की महत्वपूर्ण समय सीमा का सामना करना पड़ रहा है।\n",
    "\n",
    "ऐसा प्रतीत होता है कि डेमोक्रेट और रिपब्लिकन ने एक सतत प्रस्ताव, या सीआर पारित करने के लिए इस्तीफा दे दिया है, जो 2025 की शुरुआत में सरकार को अस्थायी रूप से वित्त पोषित करेगा - संभवतः मार्च - क्योंकि उनके पास इस वर्ष पूर्ण वित्तपोषण सौदा करने के लिए समय नहीं बचा है। दोनों पार्टियाँ नए वित्तीय वर्ष के लिए समग्र व्यय स्तर पर भी सहमत नहीं हुई हैं, सरकार के विभिन्न हिस्सों में धन कैसे आवंटित किया जाए, इसकी तो बात ही छोड़ दें।\n",
    "\n",
    "\n",
    "रिपब्लिकन के लिए, यह दोधारी तलवार है।\n",
    "\n",
    "समय सीमा तय करने में रिपब्लिकन के लिए लाभ यह है कि नए साल में सरकारी फंडिंग को आकार देने के लिए उनके पास अधिक लाभ होगा, क्योंकि निर्वाचित राष्ट्रपति डोनाल्ड ट्रम्प व्हाइट हाउस में लौट आएंगे और जीओपी सीनेट पर नियंत्रण कर लेगी और एक संकीर्ण सदन बहुमत बनाए रखेगी।\n",
    "\n",
    "बड़ा नकारात्मक पक्ष यह है कि इससे ट्रंप के राष्ट्रपति बनने की शुरुआत में ही एक महत्वपूर्ण समय सीमा तय हो जाएगी, जिससे संभावित रूप से सीनेट के माध्यम से उनके उम्मीदवारों की पुष्टि करने और रिपब्लिकन द्वारा उनके कर कटौती को बढ़ाने और उनके आव्रजन को आगे बढ़ाने के लिए बड़े पार्टी-लाइन बिल से मूल्यवान समय बर्बाद हो जाएगा। और सीमा सुरक्षा एजेंडा।\n",
    "\n",
    "\"हमें एक ही समय में बहुत सारी चीजें करनी हैं,\" हाउस मेजॉरिटी लीडर स्टीव स्कैलिस, आर-ला, ने ट्रम्प के दूसरे कार्यकाल के पहले 100 दिनों के बारे में कहा। \"हम टहलने जा रहे हैं और गम चबा रहे हैं।\"\n",
    "\n",
    "कुछ रिपब्लिकन नए ट्रम्प राष्ट्रपति पद की शुरुआत में फंडिंग की समय सीमा में नहीं फंसना पसंद करेंगे।\"\"\"\n",
    "result = pipeline(example_text)\n",
    "print(\"\\nSummary:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c599c265-f19b-4388-b990-831ed60b8d62",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated text:  WASHINGTON, DC – With Congress returning this week to make one final move before it closes, it faces a major deadline on December 20 to avoid shutting down the government. Democrats and Republicans seem resigned to pushing through a decision, or CR, that would temporarily fund the government until early 2025 – probably in March – when they run out of time to agree on full funding this year.\n",
      "\n",
      "Summary: Democrats and Republicans seem resigned to pushing through a decision that would temporarily fund the government until early 2025.\n"
     ]
    }
   ],
   "source": [
    "example_text = \"\"\"واشنطن – مع عودة الكونجرس هذا الأسبوع للقيام بموجة أخيرة من النشاط قبل أن يختتم جلسته، فإنه يواجه موعدًا نهائيًا رئيسيًا في 20 ديسمبر لتجنب إغلاق الحكومة.\n",
    "\n",
    "يبدو أن الديمقراطيين والجمهوريين مستسلمون لتمرير قرار مستمر، أو CR، من شأنه أن يمول الحكومة مؤقتًا حتى أوائل عام 2025 - على الأرجح في مارس - حيث ينفد الوقت أمامهم للتوصل إلى اتفاق تمويل كامل هذا العام. ولم يتفق الطرفان حتى على مستوى الإنفاق الإجمالي للسنة المالية الجديدة، ناهيك عن كيفية تخصيص الأموال عبر أجزاء من الحكومة.\n",
    "\n",
    "ألمح زعيم الأغلبية في مجلس الشيوخ تشاك شومر، ديمقراطي من ولاية نيويورك، إلى حتمية مشروع قانون قصير الأجل يوم الاثنين، قائلاً: \"نحن بحاجة إلى إبقاء الأحكام المثيرة للخلاف وغير الضرورية خارج أي تمديد للتمويل الحكومي، وإلا فسيكون من الصعب تمرير مشروع قانون\". CR في الوقت المناسب.\n",
    "\n",
    "وبالنسبة للجمهوريين، فإن هذا سيف ذو حدين.\n",
    "\n",
    "الجانب الإيجابي بالنسبة للجمهوريين في تحديد الموعد النهائي هو أنهم سيكون لديهم المزيد من النفوذ لتشكيل التمويل الحكومي في العام الجديد، مع عودة الرئيس المنتخب دونالد ترامب إلى البيت الأبيض وسيطرة الحزب الجمهوري على مجلس الشيوخ والحفاظ على أغلبية ضيقة في مجلس النواب.\n",
    "\n",
    "الجانب السلبي الكبير هو أنه سيحدد موعدًا نهائيًا حاسمًا في وقت مبكر من رئاسة ترامب، مما قد يستغرق وقتًا ثمينًا بعيدًا عن تأكيد مرشحيه من خلال مجلس الشيوخ ومن مشروع قانون الحزب الكبير الذي يتطلع إليه الجمهوريون لتمديد تخفيضاته الضريبية وتعزيز الهجرة. وأجندة أمن الحدود.\n",
    "\n",
    "وأضاف كينيدي أنه من المرجح أن يعلق الكونجرس \"30 [مليار دولار] إلى 40 مليار دولار من الإغاثة في حالات الكوارث\" إلى الجمهورية التشيكية، بما في ذلك تمويل الولايات التي ضربتها الأعاصير هذا العام. وقال: \"لن يكون ذلك كافيا، لكنه سيكون كافيا للبدء\".\"\"\"\n",
    "\n",
    "result = pipeline(example_text)\n",
    "print(\"\\nSummary:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "35372c14-1da9-46c6-b45b-a980c642b521",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:2817: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'num_beams': 5, 'forced_bos_token_id': 250004}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./translation_model/tokenizer_config.json',\n",
       " './translation_model/special_tokens_map.json',\n",
       " './translation_model/sentencepiece.bpe.model',\n",
       " './translation_model/added_tokens.json')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the translation model and tokenizer\n",
    "translation_model.save_pretrained(\"./translation_model\")\n",
    "translation_tokenizer.save_pretrained(\"./translation_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd742bcd-d3f3-4899-a6c3-5315ab910ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model.save_pretrained(\"./lora_summarization_model\")\n",
    "summarization_tokenizer.save_pretrained(\"./lora_summarization_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83325d2-1649-4eb0-bb03-c12ed7729b7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-cpu.2-11:m125"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
